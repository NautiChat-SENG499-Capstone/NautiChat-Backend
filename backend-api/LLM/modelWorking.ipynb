{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a370fd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from datasets import load_dataset\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the variables from .env\n",
    "\n",
    "# Access the variables using os.getenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "ONC_TOKEN = os.getenv(\"ONC_TOKEN\")\n",
    "CAMBRIDGE_LOCATION_CODE = os.getenv(\"CAMBRIDGE_LOCATION_CODE\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "model = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ceb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CurrentDate = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df = load_dataset('gsnap88/ONCKnowledgeBase', split='train').to_pandas()\n",
    "\n",
    "\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    text = f\"Category: {row['category']}\\nEntry: {row['entry']}\\nEntry Contents: {row['entryContents']}\"\n",
    "    document = Document(page_content=text)\n",
    "    documents.append(document)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name  = \"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"reranker\",\n",
    ")\n",
    "\n",
    "retriever = qdrant.as_retriever(search_kwargs = {'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(question):\n",
    "  compression_documents = compression_retriever.invoke(question)\n",
    "  compression_contents = [doc.page_content for doc in compression_documents]\n",
    "  df = pd.DataFrame({'contents': compression_contents})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_temperature(location: str):\n",
    "    # This is a mock tool/function. In a real scenario, you would call a weather API.\n",
    "    temperatures = {\"New York\": \"22째C\", \"London\": \"18째C\", \"Tokyo\": \"26째C\", \"Sydney\": \"20째C\"}\n",
    "    return temperatures.get(location, \"Temperature data not available\")\n",
    "\n",
    "async def get_weather_condition(location: str):\n",
    "    # This is a mock tool/function. In a real scenario, you would call a weather API.\n",
    "    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n",
    "    return conditions.get(location, \"Weather condition data not available\")\n",
    "\n",
    "async def get_properties_at_cambridge_bay():\n",
    "    \"\"\"Get a list of properties of data available at Cambridge Bay\n",
    "        Returns a list of dictionaries turned into a string.\n",
    "        Each Item in the list includes:\n",
    "        - description (str): Description of the property. The description may have a colon in it.\n",
    "        - propertyCode (str): Property Code of the property\n",
    "        example: '{\"Description of the property\": Property Code of the property}'\n",
    "    \"\"\"\n",
    "    property_API = f\"https://data.oceannetworks.ca/api/properties?locationCode={CAMBRIDGE_LOCATION_CODE}&token={ONC_TOKEN}\"\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(property_API)\n",
    "        response.raise_for_status() # Error handling\n",
    "\n",
    "        # Convert from JSON to Python dictionary for cleanup, return as JSON string\n",
    "        raw_data = response.json()\n",
    "        list_of_dicts = [\n",
    "            {\n",
    "                \"description\": item[\"description\"],\n",
    "                \"propertyCode\": item[\"propertyCode\"]\n",
    "            } for item in raw_data\n",
    "        ]\n",
    "        return json.dumps(list_of_dicts)\n",
    "\n",
    "async def get_daily_sea_temperature_stats_cambridge_bay(day_str: str):\n",
    "    \"\"\"\n",
    "    Get daily sea temperature statistics for Cambridge Bay\n",
    "    Args:\n",
    "        day_str (str): Date in YYYY-MM-DD format\n",
    "    \"\"\"\n",
    "    # Parse into datetime object to add 1 day (accounts for 24-hour period)\n",
    "    date_to = datetime.strptime(day_str, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    date_to_str: str = date_to.strftime(\"%Y-%m-%d\") # Convert back to string\n",
    "    print(day_str)\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Get the data from ONC API\n",
    "        temp_api = f\"https://data.oceannetworks.ca/api/scalardata/location?locationCode={CAMBRIDGE_LOCATION_CODE}&deviceCategoryCode=CTD&propertyCode=seawatertemperature&dateFrom={day_str}&dateTo={date_to_str}&rowLimit=80000&outputFormat=Object&resamplePeriod=86400&token={ONC_TOKEN}\"\n",
    "        response = await client.get(temp_api)\n",
    "        response.raise_for_status() # Error handling\n",
    "        response = response.json()\n",
    "\n",
    "    if response[\"sensorData\"] is None:\n",
    "      return ''\n",
    "      return json.dumps({\"result\": \"No data available for the given date.\"})\n",
    "\n",
    "    data = response[\"sensorData\"][0][\"data\"][0]\n",
    "\n",
    "    # Get min, max, and average and store in dictionary\n",
    "    return json.dumps({\n",
    "        \"daily_min\": round(data[\"minimum\"], 2),\n",
    "        \"daily_max\": round(data[\"maximum\"], 2),\n",
    "        \"daily_avg\": round(data[\"value\"], 2),\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0490bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_conversation(user_prompt):\n",
    "    # Initialize the conversation with system and user messages\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an assistant for Oceans Network Canada that helps users access ONCs database via natural language.  \\\n",
    "            You can choose to use the given tools to obtain the data needed to answer the prompt and provide the results if that is required. \\\n",
    "            The current day is: {CurrentDate}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "        {\"role\": \"system\",\n",
    "        \"content\": \"\"#Where Data retrieval from Vector DB will occur and be stored\n",
    "        }\n",
    "    ]\n",
    "    # Define the available tools (i.e. functions) for our model to use\n",
    "    tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature\",\n",
    "            \"description\": \"Get the temperature for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_condition\",\n",
    "            \"description\": \"Get the weather condition for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "            'name': 'get_properties_at_cambridge_bay',\n",
    "            'description': 'Get a list of properties of data available at Cambridge Bay. The function returns a list of dictionaries. Each Item in the list includes:\\n        - description (str): Description of the property. The description may have a colon in it.\\n        - propertyCode (str): Property Code of the property\\n',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                  },\n",
    "                }\n",
    "            }\n",
    "      },\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_daily_sea_temperature_stats_cambridge_bay',\n",
    "          'description': 'Get daily sea temperature statistics for Cambridge Bay\\nArgs:\\n    day_str (str): Date in YYYY-MM-DD format',\n",
    "          'parameters': {\n",
    "              'properties': {\n",
    "                  'day_str': {\n",
    "                      'type': 'string',\n",
    "                      'description': 'Date in YYYY-MM-DD format for when daily sea temperature is wanted for'\n",
    "                      }\n",
    "                  },\n",
    "            'required': ['day_str'],\n",
    "            'type': 'object'\n",
    "            }\n",
    "          }\n",
    "     }\n",
    "]\n",
    "    vectorDBResponse = get_documents(user_prompt)\n",
    "    messages[2] = ({\"role\": \"system\", \"content\": vectorDBResponse.to_string()})\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_completion_tokens=4096, # Maximum number of tokens to allow in our response\n",
    "        temperature=0.5 #A temperature of 1=default balance between randomnes and confidence. Less than 1 is less randomness, Greater than is more randomness\n",
    "    )\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Define the available tools that can be called by the LLM\n",
    "        available_functions = {\n",
    "            \"get_properties_at_cambridge_bay\": get_properties_at_cambridge_bay,\n",
    "            \"get_weather_condition\": get_weather_condition,\n",
    "            \"get_temperature\": get_temperature,\n",
    "            \"get_daily_sea_temperature_stats_cambridge_bay\": get_daily_sea_temperature_stats_cambridge_bay,\n",
    "        }\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Call the tool and get the response\n",
    "            function_response = await function_to_call(**function_args)\n",
    "            # pprint(function_args.get(\"expression\"))\n",
    "            # if function_args.get(\"expression\"):\n",
    "            #     function_response = await function_to_call(\n",
    "            #         expression=function_args.get(\"expression\")\n",
    "            #     )\n",
    "            # else:\n",
    "            #   function_response = await function_to_call()\n",
    "            # Add the tool response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "            #pprint.pprint(messages)\n",
    "        # Make a second API call with the updated conversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_completion_tokens=4096,\n",
    "            temperature=0.5\n",
    "        )#Calls LLM again with all the data from all functions\n",
    "        # Return the final response\n",
    "        return second_response.choices[0].message.content\n",
    "    else:\n",
    "        return response_message.content\n",
    "user_prompt = \"can by-catch species survive after they have been released?\"\n",
    "response = await run_conversation(user_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
